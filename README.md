# BERT Emotion Classification with Quantization

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)

Implementation of BERT quantization techniques for emotion classification on the [dair-ai/emotion](https://huggingface.co/datasets/dair-ai/emotion) dataset. This project explores model compression through Post-Training Quantization (PTQ), Quantization-Aware Training (QAT), and Quantized Low-Rank Adaptation (QLoRA).

## üéØ Project Overview

This repository contains implementations of various BERT compression techniques to reduce model size and improve inference speed while maintaining accuracy on emotion classification tasks.

**Task**: Multi-class emotion classification (6 classes: sadness, joy, love, anger, fear, surprise)

**Dataset**: [dair-ai/emotion](https://huggingface.co/datasets/dair-ai/emotion)
- Train: 16,000 samples
- Validation: 2,000 samples  
- Test: 2,000 samples

## üìä Results Summary

> **Status**: üöß Work in Progress - Will be updated as models are trained

| Model Variant | Macro F1 | Accuracy | Size (MB) | Latency (ms) | Status |
|---------------|----------|----------|-----------|--------------|---------|
| Baseline (FP32) | TBD | TBD | TBD | TBD | ‚è≥ Pending |
| PTQ (INT8) | TBD | TBD | TBD | TBD | ‚è≥ Pending |
| QAT (INT8) | TBD | TBD | TBD | TBD | ‚è≥ Pending |
| QLoRA | TBD | TBD | TBD | TBD | ‚è≥ Pending |

## üóÇÔ∏è Repository Structure
```
bert-emotion-quantization/
‚îÇ
‚îú‚îÄ‚îÄ weights/                          # Model weights (uploaded as trained)
‚îÇ   ‚îú‚îÄ‚îÄ baseline/                     # ‚è≥ Coming soon
‚îÇ   ‚îú‚îÄ‚îÄ ptq/                          # ‚è≥ Coming soon
‚îÇ   ‚îú‚îÄ‚îÄ qat/                          # ‚è≥ Coming soon
‚îÇ   ‚îî‚îÄ‚îÄ qlora/                        # ‚è≥ Coming soon
‚îÇ
‚îú‚îÄ‚îÄ configs/                          # Configuration files
‚îÇ   ‚îî‚îÄ‚îÄ training_config.json          # ‚è≥ Coming soon
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                        # Colab notebooks
‚îÇ   ‚îî‚îÄ‚îÄ inference_evaluation.ipynb    # ‚è≥ Coming soon
‚îÇ
‚îú‚îÄ‚îÄ results/                          # Evaluation results
‚îÇ   ‚îú‚îÄ‚îÄ confusion_matrices/           # ‚è≥ Coming soon
‚îÇ   ‚îî‚îÄ‚îÄ metrics/                      # ‚è≥ Coming soon
‚îÇ
‚îú‚îÄ‚îÄ docs/                             # Documentation
‚îÇ   ‚îî‚îÄ‚îÄ report.pdf                    # ‚è≥ Coming soon
‚îÇ
‚îú‚îÄ‚îÄ README.md                         # This file
‚îî‚îÄ‚îÄ requirements.txt                  # ‚è≥ Coming soon
```

## üöÄ Quick Start

### Prerequisites
```bash
Python 3.8+
CUDA-capable GPU (recommended)
Google Colab account (for notebook execution)
```

### Installation
```bash
# Clone the repository
git clone https://github.com/shailesh22290/bert-emotion-quantization.git
cd bert-emotion-quantization

# Install dependencies (once requirements.txt is added)
pip install -r requirements.txt
```

### Running Inference

> **Note**: Colab notebook will be available soon. It will automatically download weights from this repository.
```bash
# Instructions will be added once the notebook is ready
# Expected runtime: ‚â§15 minutes per model variant on Colab GPU
```

## üìù Implementation Details

### 1Ô∏è‚É£ Baseline Model (FP32/FP16)
**Status**: ‚è≥ In Progress

- **Model**: `bert-base-uncased`
- **Training**:
  - Epochs: TBD
  - Learning rate: TBD
  - Batch size: TBD
  - Max sequence length: TBD
- **Performance**: TBD

### 2Ô∏è‚É£ Post-Training Quantization (PTQ)
**Status**: ‚è≥ Pending

- **Quantization method**: TBD (Dynamic/Static)
- **Bit-width**: INT8
- **Layers quantized**: TBD
- **Calibration**: TBD
- **Performance**: TBD

### 3Ô∏è‚É£ Quantization-Aware Training (QAT)
**Status**: ‚è≥ Pending

- **QAT configuration**: TBD
- **Frozen layers**: TBD
- **Training strategy**: TBD
- **Performance**: TBD

### 4Ô∏è‚É£ Quantized LoRA (QLoRA)
**Status**: ‚è≥ Pending

- **Base quantization**: TBD
- **LoRA rank (r)**: TBD
- **Alpha**: TBD
- **Target modules**: TBD
- **Performance**: TBD

## üìà Evaluation Metrics

All models are evaluated on:
- ‚úÖ Macro F1-score (primary metric)
- ‚úÖ Accuracy
- ‚úÖ Per-class F1 scores
- ‚úÖ Confusion matrix
- ‚úÖ Model size (MB)
- ‚úÖ Inference latency (ms/example)

## üîÑ Project Timeline

### Phase 1: Setup & Baseline ‚úÖ Current Phase
- [x] Repository setup
- [x] README creation
- [ ] Dataset exploration
- [ ] Baseline model training

### Phase 2: PTQ Implementation ‚è≥ Upcoming
- [ ] PTQ implementation
- [ ] Calibration & evaluation
- [ ] Weight upload

### Phase 3: QAT Implementation ‚è≥ Upcoming
- [ ] QAT setup
- [ ] Training with quantization simulation
- [ ] Evaluation & comparison

### Phase 4: QLoRA Implementation ‚è≥ Upcoming
- [ ] QLoRA configuration
- [ ] Adapter training
- [ ] Final evaluation

### Phase 5: Analysis & Documentation ‚è≥ Upcoming
- [ ] Comprehensive comparison
- [ ] Report writing
- [ ] Notebook finalization

**Expected Completion**: November 20, 2025

## üõ†Ô∏è Technologies Used

- **Framework**: PyTorch, Hugging Face Transformers
- **Quantization**: `torch.quantization`, `torch.ao.quantization`
- **LoRA**: PEFT library, LLaMA Factory
- **Dataset**: Hugging Face Datasets
- **Evaluation**: scikit-learn, seaborn

## üìö References

- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)
- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
- [PyTorch Quantization Documentation](https://pytorch.org/docs/stable/quantization.html)
- [dair-ai/emotion Dataset](https://huggingface.co/datasets/dair-ai/emotion)



## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.

## üôè Acknowledgments

- Course Instructor: Dr. Jasabanta Patro
- Hugging Face for datasets and model hosting
- PyTorch team for quantization tools

---

**Last Updated**: 22nd October 2025  
**Status**: üöß Active Development

> ‚≠ê Star this repository if you find it helpful!
